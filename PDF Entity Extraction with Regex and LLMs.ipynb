{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "952f253b",
   "metadata": {},
   "source": [
    "# Sample Notebook for Extracting Data from OCRed PDFs Using Regex and LLMs\n",
    "\n",
    "One can use this notebook to build a pipeline to parse and extract data from OCRed PDF files. **Warining:** When using LLMs for entity extraction, be sure to perform extensive quality control. They are very susceptible to distracting language (latching on to text that sound \"kind of like\" what you're looking for) and missing language (making up content to fill any holes), and importantly, they do **NOT** provide any hints to when they may be erroring. \n",
    "\n",
    "First we load the libraries we need. Note, if you try to run the cell, and you get something like `ModuleNotFoundError: No module named 'mod_name'`, you'll need to install the module. You can do this uncommenting the line bellow that reads `#!pip install mod_name` if it's listed. If it isn't, you can probably install it with a similarly formatted command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b08c931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os\n",
    "#!pip install PyPDF2\n",
    "#!pip install re\n",
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "\n",
    "import os\n",
    "from os import walk, path\n",
    "import PyPDF2\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_pdf(file):\n",
    "    try:\n",
    "        pdfFile = PyPDF2.PdfFileReader(open(file, \"rb\"), strict=False)\n",
    "        text = \"\"\n",
    "        for page in pdfFile.pages:\n",
    "            text += \" \" + page.extractText()\n",
    "        return text\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea2e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Audio call\n",
    "# Only works on Mac. If you aren't using a Mac, you should disable such calls below.\n",
    "tmp = os.system( \"say Testing, testing, one, two, three.\")\n",
    "del(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e90a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install openai\n",
    "#!pip install tiktoken\n",
    "\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import openai\n",
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "import tiktoken\n",
    "ENCODING = \"gpt2\"\n",
    "encoding = tiktoken.get_encoding(ENCODING)\n",
    "\n",
    "def complete_text(prompt,temp=0,trys=0,clean=True):\n",
    "    \n",
    "    global tokens_used\n",
    "    \n",
    "    model=\"text-davinci-003\"\n",
    "    model_token_limit = 4097\n",
    "    \n",
    "    token_count = len(encoding.encode(prompt))\n",
    "    max_tokens= model_token_limit-round(token_count+5)\n",
    "    \n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "          model=model,\n",
    "          prompt=prompt,\n",
    "          temperature=temp,\n",
    "          max_tokens=max_tokens,\n",
    "          top_p=1.0,\n",
    "          frequency_penalty=0.0,\n",
    "          presence_penalty=0.0\n",
    "        )\n",
    "        output = str(response[\"choices\"][0][\"text\"].strip())\n",
    "    except:\n",
    "        print(\"Problem with API call!\")\n",
    "        output = \"\"\"{\"output\":\"error\"}\"\"\"\n",
    "        \n",
    "    tokens_used += token_count+len(encoding.encode(output))\n",
    "    \n",
    "    if clean:\n",
    "        return clean_pseudo_json(output,temp=0,trys=trys)\n",
    "    else:\n",
    "        return output\n",
    "    \n",
    "def clean_pseudo_json(string,temp=0,key=\"output\",trys=0,ask_for_help=1):\n",
    "    try:\n",
    "        output = json.loads(string)[key]\n",
    "    except:\n",
    "        try:\n",
    "            string_4_json = re.findall(\"\\{.*\\}\",re.sub(\"\\n\",\"\",string))[0]\n",
    "            output = json.loads(string_4_json)[key]\n",
    "        except:\n",
    "            try:\n",
    "                string = \"{\"+string+\"}\"\n",
    "                string_4_json = re.findall(\"\\{.*\\}\",re.sub(\"\\n\",\"\",string))[0]\n",
    "                output = json.loads(string_4_json)[key]\n",
    "            except Exception as e:\n",
    "                prompt = \"I tried to parse some json and got this error, '{}'. This was the would-be json.\\n\\n{}\\n\\nReformat it to fix the error.\".format(e,string)              \n",
    "                if trys <= 3:\n",
    "                    if trys == 0:\n",
    "                        warm_up = 0\n",
    "                    else:\n",
    "                        warm_up = 0.25\n",
    "                    output = complete_text(prompt,temp=0+warm_up,trys=trys+1)  \n",
    "                    print(\"\\n\"+str(output)+\"\\n\")            \n",
    "                elif ask_for_help==1:\n",
    "                    print(prompt+\"\\nReformaing FAILED!!!\")\n",
    "                    try:\n",
    "                        os.system( \"say hey! I need some help. A little help please?\")\n",
    "                    except:\n",
    "                        print(\"'say' not supported.\\n\\n\")\n",
    "                    output = input(\"Let's see if we can avoid being derailed. Examine the above output and construct your own output text. Then enter it below. If the output needs to be something other than a string, e.g., a list or json, start it with `EVAL: `. If you're typing that, be very sure there's no malicious code in the output.\\n\")      \n",
    "                    if output[:6]==\"EVAL: \":\n",
    "                        output = eval(output[6:])\n",
    "                else:\n",
    "                    output = \"There was an error getting a reponse!\"\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e011e2f",
   "metadata": {},
   "source": [
    "This notebook will make use of the [OpenAI API](https://openai.com/blog/openai-api). For things to work, you'll need to make sure that the files referenced below exist and contain your relevant credintials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af39423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../keys_work/openai_org.txt\", \"r\") as file:\n",
    "    openai.organization = file.read().rstrip()\n",
    "with open(\"../keys_work/openai_key.txt\", \"r\") as file:\n",
    "    openai.api_key = file.read().rstrip()\n",
    "\n",
    "llm_temperature = 0 # I strongly suggest keeping the LLM's temp at zero to avoid it making things up.\n",
    "\n",
    "# Toggle LLM usage on or off\n",
    "use_LLM = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eae960",
   "metadata": {},
   "source": [
    "Next, place a bunch of OCRed pdf files in the right folder (here, the `data/boston` folder). FWIW, you can use Adobe Pro to OCR in batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeaf722",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ~1135 words from: \"data/boston/109 to 117A Blue Hill Ave_BOA848024_Decision.pdf\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame() #this will create an empty dataframe\n",
    "\n",
    "path = \"data/boston/\" # this is where we'll be looking for files\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(path): # create a list of file names\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "\n",
    "token_counts = []\n",
    "for file in f: # for each file in the list of file names, do some stuff\n",
    "    \n",
    "    tokens_used = 0\n",
    "    \n",
    "    column_names = [\"file\"]\n",
    "    column_values = [file]\n",
    "\n",
    "    fileloc = path+file\n",
    "    text = read_pdf(fileloc)\n",
    "    words = len(text.split())\n",
    "    \n",
    "    print(\"Reading ~{} words from: \\\"{}\\\"\\n\".format(words,fileloc))\n",
    "    \n",
    "    #############################################################\n",
    "    # Here's where we use regex to pull out specific content\n",
    "    \n",
    "    # ---------------------------------------------------------       \n",
    "    # case Number\n",
    "    # ---------------------------------------------------------        \n",
    "    case_no = re.search(\"(?<=case no\\.\\s)(.*?)(?=permit#)\",text, flags=re.IGNORECASE).groups(0)[0].strip()\n",
    "    column_names.append(\"case_no\")\n",
    "    column_values.append(case_no)\n",
    "    \n",
    "    # ---------------------------------------------------------        \n",
    "    # address\n",
    "    # ---------------------------------------------------------        \n",
    "    address = re.search(\"(?<=concerning premises\\s)(.*?)(?=\\s?,?\\s?ward)\",text, flags=re.IGNORECASE).groups(0)[0].strip()\n",
    "    column_names.append(\"address\")\n",
    "    column_values.append(address)\n",
    "    \n",
    "    # ---------------------------------------------------------        \n",
    "    # ward\n",
    "    # ---------------------------------------------------------        \n",
    "    ward = re.search(\"(?<=ward\\s)(.*?)(?=to vary)\",text, flags=re.IGNORECASE).groups(0)[0].strip()\n",
    "    column_names.append(\"ward\")\n",
    "    column_values.append(ward)\n",
    "    \n",
    "    #############################################################\n",
    "    # Here's where use GPT to pull out some specific content. \n",
    "    \n",
    "    #\n",
    "    # Note: You should consider combining multiple prompts into a single prompt \n",
    "    # to avoid making unnecessary api calls. \n",
    "    #\n",
    "  \n",
    "    \n",
    "    if use_LLM:\n",
    "    \n",
    "        # ---------------------------------------------------------    \n",
    "        # description of variance requested\n",
    "        # ---------------------------------------------------------        \n",
    "        prompt_text = \"\"\"Below you will be provided with the text of an order from a local zoning board of appeals responding to a variance request. You're looking to find the _description of variance requested_. That is, what the petitioner was asking for.        \n",
    "\n",
    "    Here's the text of the order. \n",
    "\n",
    "    {}\n",
    "\n",
    "    ---\n",
    "\n",
    "    Return a json object, including the outermost currly brakets, where the key is \"output\" and the value is a the _description of variance requested_. If you can't find a _description of variance requested_ in the text of the above, answer \"none found\". Be sure to use valid json, encasing keys and values in double quotes, and escaping internal quotes and special characters as needed.\"\"\". format(text)\n",
    "        #print(prompt_text)    \n",
    "        request = complete_text(prompt_text,temp=llm_temperature)\n",
    "        column_names.append(\"request\")\n",
    "        column_values.append(request)\n",
    "\n",
    "        # ---------------------------------------------------------   \n",
    "        # relevant facts\n",
    "        # ---------------------------------------------------------        \n",
    "        prompt_text = \"\"\"Below you will be provided with the text of an order from a local zoning board of appeals responding to a variance request. You're looking to find the _relevant facts_. That is, what facts did the board needed to know to rule on the petitioner's request.\n",
    "\n",
    "    Here's the text of the order. \n",
    "\n",
    "    {}\n",
    "\n",
    "    ---\n",
    "\n",
    "    Return a json object, including the outermost currly brakets, where the key is \"output\" and the value is a a short summary of the _relevant facts_. If you can't find _relevant facts_ in the text of the above, answer \"none found\". Be sure to use valid json, encasing keys and values in double quotes, and escaping internal quotes and special characters as needed.\"\"\". format(text)\n",
    "        #print(prompt_text)    \n",
    "        facts = complete_text(prompt_text,temp=llm_temperature)\n",
    "        column_names.append(\"facts\")\n",
    "        column_values.append(facts)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # decision/reasoning & short decision\n",
    "        # ---------------------------------------------------------        \n",
    "        prompt_text = \"\"\"Below you will be provided with the text of an order from a local zoning board of appeals responding to a variance request. You're looking to find the board's _decision_ and _reasoning_. That is, how the board ruled on the petitioner's request and how it came to that decision.        \n",
    "\n",
    "    Here's the text of the order. \n",
    "\n",
    "    {}\n",
    "\n",
    "    ---\n",
    "\n",
    "    Return a json object, including the outermost currly brakets, where the key is \"output\" and the value is a json object with two key-value pairs: (1) the first item has the key \"reasoning\" and the value is a summary of the board's _reasoning_ as stated above; and (2) the second item has a the key \"decision\" with a value that is a one or two word re-statment of the _decision_ found above (e.g., \"granted,\" \"not granted,\" or \"granted in part\"). If you can't find the _decision_ or _reasoning_ in the text of the above order, both values should read \"none found\". Be sure to use valid json, encasing keys and values in double quotes, and escaping internal quotes and special characters as needed.\"\"\". format(text)\n",
    "        #print(prompt_text)    \n",
    "        output = complete_text(prompt_text,temp=llm_temperature)\n",
    "        \n",
    "        reasoning = output[\"reasoning\"]\n",
    "        column_names.append(\"reasoning\")\n",
    "        column_values.append(reasoning)\n",
    "\n",
    "        decision = output[\"decision\"]\n",
    "        column_names.append(\"decision\")\n",
    "        column_values.append(decision)\n",
    "\n",
    "    #############################################################   \n",
    "\n",
    "    i = 0\n",
    "    for datum in column_values:\n",
    "        print(\"{}: {}\\n\".format(column_names[i].upper(),datum))\n",
    "        i+=1\n",
    "\n",
    "    print(\"Tokens used (approx.): {} (API Cost ${})\\n\".format(tokens_used,tokens_used*(0.02/1000))) # See https://openai.com/pricing\n",
    "    token_counts.append(tokens_used)\n",
    "        \n",
    "    print(\"================================================\\n\")\n",
    "    \n",
    "    df = df.append(pd.DataFrame([column_values],columns=column_names), ignore_index=True,sort=False)\n",
    "\n",
    "print(\"Average approx. tokens used per item {} (API Cost ${})\".format(np.array(token_counts).mean(),np.array(token_counts).mean()*(0.02/1000)))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aeef32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're happy with the stuff you pulled out above, you can write the df to a csv file\n",
    "\n",
    "df.to_csv(\"data/Coding of Boston Variance Decisions.csv\", index=False, encoding=\"utf-8\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
